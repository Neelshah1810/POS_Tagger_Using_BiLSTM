{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install torch numpy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7xDCnabJe5v",
        "outputId": "8fb1ca92-2422-4cd7-9524-d4fd97855df4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-xZMg-PJSlT",
        "outputId": "f3d392dc-f191-4927-f1fc-ad4457130749"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab\n",
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "Memory Available: 15.83 GB\n",
            "\n",
            "Select language:\n",
            "1. Hindi\n",
            "2. English\n",
            "3. Both\n",
            "\n",
            "[Auto-selecting Both languages for Colab demo]\n",
            "\n",
            "============================================================\n",
            "Processing Hindi\n",
            "============================================================\n",
            "UD_Hindi-HDTB already exists, skipping download.\n",
            "\n",
            "Found files:\n",
            "  dev: UD_Hindi-HDTB/hi_hdtb-ud-dev.conllu\n",
            "  test: UD_Hindi-HDTB/hi_hdtb-ud-test.conllu\n",
            "  train: UD_Hindi-HDTB/hi_hdtb-ud-train.conllu\n",
            "\n",
            "Loading datasets...\n",
            "Train sentences: 13306\n",
            "Dev sentences: 1659\n",
            "Test sentences: 1684\n",
            "\n",
            "Building vocabulary...\n",
            "Vocabulary size: 9629\n",
            "Number of POS tags: 17\n",
            "POS tags: ['<PAD>', 'DET', 'PROPN', 'ADP', 'ADV', 'ADJ', 'NOUN', 'NUM', 'AUX', 'PUNCT', 'PRON', 'VERB', 'CCONJ', 'PART', 'SCONJ', 'X', 'INTJ']\n",
            "\n",
            "Initializing model...\n",
            "Model parameters: 5,185,681\n",
            "\n",
            "Starting training...\n",
            "Epoch 1/20 - Loss: 0.9156 - Dev Acc: 0.8437\n",
            "  → New best model saved!\n",
            "Epoch 2/20 - Loss: 0.4786 - Dev Acc: 0.8873\n",
            "  → New best model saved!\n",
            "Epoch 3/20 - Loss: 0.3757 - Dev Acc: 0.9124\n",
            "  → New best model saved!\n",
            "Epoch 4/20 - Loss: 0.3209 - Dev Acc: 0.9235\n",
            "  → New best model saved!\n",
            "Epoch 5/20 - Loss: 0.2819 - Dev Acc: 0.9317\n",
            "  → New best model saved!\n",
            "Epoch 6/20 - Loss: 0.2542 - Dev Acc: 0.9370\n",
            "  → New best model saved!\n",
            "Epoch 7/20 - Loss: 0.2318 - Dev Acc: 0.9396\n",
            "  → New best model saved!\n",
            "Epoch 8/20 - Loss: 0.2143 - Dev Acc: 0.9448\n",
            "  → New best model saved!\n",
            "Epoch 9/20 - Loss: 0.1991 - Dev Acc: 0.9447\n",
            "Epoch 10/20 - Loss: 0.1866 - Dev Acc: 0.9480\n",
            "  → New best model saved!\n",
            "Epoch 11/20 - Loss: 0.1764 - Dev Acc: 0.9499\n",
            "  → New best model saved!\n",
            "Epoch 12/20 - Loss: 0.1661 - Dev Acc: 0.9514\n",
            "  → New best model saved!\n",
            "Epoch 13/20 - Loss: 0.1574 - Dev Acc: 0.9525\n",
            "  → New best model saved!\n",
            "Epoch 14/20 - Loss: 0.1486 - Dev Acc: 0.9526\n",
            "  → New best model saved!\n",
            "Epoch 15/20 - Loss: 0.1415 - Dev Acc: 0.9537\n",
            "  → New best model saved!\n",
            "Epoch 16/20 - Loss: 0.1346 - Dev Acc: 0.9537\n",
            "  → New best model saved!\n",
            "Epoch 17/20 - Loss: 0.1297 - Dev Acc: 0.9549\n",
            "  → New best model saved!\n",
            "Epoch 18/20 - Loss: 0.1245 - Dev Acc: 0.9558\n",
            "  → New best model saved!\n",
            "Epoch 19/20 - Loss: 0.1206 - Dev Acc: 0.9561\n",
            "  → New best model saved!\n",
            "Epoch 20/20 - Loss: 0.1156 - Dev Acc: 0.9560\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Accuracy: 0.9568\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         DET       0.96      0.97      0.97       745\n",
            "       PROPN       0.90      0.89      0.89      4438\n",
            "         ADP       1.00      0.99      1.00      7464\n",
            "         ADV       0.90      0.89      0.89       304\n",
            "         ADJ       0.89      0.93      0.91      2129\n",
            "        NOUN       0.93      0.93      0.93      8036\n",
            "         NUM       0.95      0.94      0.94       693\n",
            "         AUX       0.97      0.99      0.98      2392\n",
            "       PUNCT       1.00      1.00      1.00      2420\n",
            "        PRON       0.99      0.98      0.98      1372\n",
            "        VERB       0.97      0.96      0.97      3461\n",
            "       CCONJ       0.99      1.00      0.99       635\n",
            "        PART       0.99      0.98      0.99       677\n",
            "       SCONJ       0.99      0.99      0.99       655\n",
            "           X       0.50      0.11      0.18         9\n",
            "\n",
            "    accuracy                           0.96     35430\n",
            "   macro avg       0.93      0.90      0.91     35430\n",
            "weighted avg       0.96      0.96      0.96     35430\n",
            "\n",
            "\n",
            "Example Predictions:\n",
            "\n",
            "Word\t\tTrue POS\tPredicted POS\n",
            "--------------------------------------------------\n",
            "इसके           \tPRON      \tPRON      \n",
            "अतिरिक्त       \tADP       \tADP       \n",
            "गुग्गुल        \tPROPN     \tADJ       \n",
            "कुंड           \tPROPN     \tNOUN      \n",
            ",              \tPUNCT     \tPUNCT     \n",
            "भीम            \tPROPN     \tNOUN      \n",
            "गुफा           \tPROPN     \tNOUN      \n",
            "तथा            \tCCONJ     \tCCONJ     \n",
            "भीमशिला        \tPROPN     \tNOUN      \n",
            "भी             \tPART      \tPART      \n",
            "दर्शनीय        \tADJ       \tADJ       \n",
            "स्थल           \tNOUN      \tNOUN      \n",
            "हैं            \tAUX       \tAUX       \n",
            "।              \tPUNCT     \tPUNCT     \n",
            "\n",
            "============================================================\n",
            "Processing English\n",
            "============================================================\n",
            "Downloading https://github.com/UniversalDependencies/UD_English-GUM.git...\n",
            "Downloaded to UD_English-GUM\n",
            "\n",
            "Found files:\n",
            "  train: UD_English-GUM/en_gum-ud-train.conllu\n",
            "  dev: UD_English-GUM/en_gum-ud-dev.conllu\n",
            "  test: UD_English-GUM/en_gum-ud-test.conllu\n",
            "\n",
            "Loading datasets...\n",
            "Train sentences: 10224\n",
            "Dev sentences: 1575\n",
            "Test sentences: 1464\n",
            "\n",
            "Building vocabulary...\n",
            "Vocabulary size: 8149\n",
            "Number of POS tags: 18\n",
            "POS tags: ['<PAD>', 'ADJ', 'NOUN', 'CCONJ', 'PUNCT', 'ADP', 'PROPN', 'VERB', 'ADV', 'AUX', 'DET', 'PRON', 'SCONJ', 'X', 'SYM', 'PART', 'NUM', 'INTJ']\n",
            "\n",
            "Initializing model...\n",
            "Model parameters: 4,996,754\n",
            "\n",
            "Starting training...\n",
            "Epoch 1/20 - Loss: 1.1550 - Dev Acc: 0.8100\n",
            "  → New best model saved!\n",
            "Epoch 2/20 - Loss: 0.6273 - Dev Acc: 0.8478\n",
            "  → New best model saved!\n",
            "Epoch 3/20 - Loss: 0.5177 - Dev Acc: 0.8731\n",
            "  → New best model saved!\n",
            "Epoch 4/20 - Loss: 0.4551 - Dev Acc: 0.8866\n",
            "  → New best model saved!\n",
            "Epoch 5/20 - Loss: 0.4075 - Dev Acc: 0.8975\n",
            "  → New best model saved!\n",
            "Epoch 6/20 - Loss: 0.3737 - Dev Acc: 0.9032\n",
            "  → New best model saved!\n",
            "Epoch 7/20 - Loss: 0.3478 - Dev Acc: 0.9073\n",
            "  → New best model saved!\n",
            "Epoch 8/20 - Loss: 0.3212 - Dev Acc: 0.9090\n",
            "  → New best model saved!\n",
            "Epoch 9/20 - Loss: 0.3046 - Dev Acc: 0.9130\n",
            "  → New best model saved!\n",
            "Epoch 10/20 - Loss: 0.2864 - Dev Acc: 0.9181\n",
            "  → New best model saved!\n",
            "Epoch 11/20 - Loss: 0.2724 - Dev Acc: 0.9191\n",
            "  → New best model saved!\n",
            "Epoch 12/20 - Loss: 0.2599 - Dev Acc: 0.9213\n",
            "  → New best model saved!\n",
            "Epoch 13/20 - Loss: 0.2442 - Dev Acc: 0.9218\n",
            "  → New best model saved!\n",
            "Epoch 14/20 - Loss: 0.2322 - Dev Acc: 0.9247\n",
            "  → New best model saved!\n",
            "Epoch 15/20 - Loss: 0.2242 - Dev Acc: 0.9262\n",
            "  → New best model saved!\n",
            "Epoch 16/20 - Loss: 0.2148 - Dev Acc: 0.9281\n",
            "  → New best model saved!\n",
            "Epoch 17/20 - Loss: 0.2050 - Dev Acc: 0.9286\n",
            "  → New best model saved!\n",
            "Epoch 18/20 - Loss: 0.1961 - Dev Acc: 0.9289\n",
            "  → New best model saved!\n",
            "Epoch 19/20 - Loss: 0.1885 - Dev Acc: 0.9306\n",
            "  → New best model saved!\n",
            "Epoch 20/20 - Loss: 0.1836 - Dev Acc: 0.9311\n",
            "  → New best model saved!\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Accuracy: 0.9229\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.82      0.88      0.85      1833\n",
            "        NOUN       0.87      0.88      0.88      4906\n",
            "       CCONJ       0.99      0.99      0.99       988\n",
            "       PUNCT       1.00      1.00      1.00      3550\n",
            "         ADP       0.97      0.97      0.97      2865\n",
            "       PROPN       0.70      0.71      0.70      1787\n",
            "        VERB       0.91      0.90      0.91      3020\n",
            "         ADV       0.92      0.88      0.90      1336\n",
            "         AUX       0.98      0.99      0.98      1533\n",
            "         DET       0.99      0.99      0.99      2521\n",
            "        PRON       0.99      0.99      0.99      2187\n",
            "       SCONJ       0.87      0.86      0.87       468\n",
            "           X       0.75      0.18      0.29        33\n",
            "         SYM       0.87      0.75      0.81        36\n",
            "        PART       0.96      0.98      0.97       671\n",
            "         NUM       0.95      0.82      0.88       479\n",
            "        INTJ       0.86      0.87      0.86       184\n",
            "\n",
            "    accuracy                           0.92     28397\n",
            "   macro avg       0.91      0.86      0.87     28397\n",
            "weighted avg       0.92      0.92      0.92     28397\n",
            "\n",
            "\n",
            "Example Predictions:\n",
            "\n",
            "Word\t\tTrue POS\tPredicted POS\n",
            "--------------------------------------------------\n",
            "The            \tDET       \tDET       \n",
            "prevalence     \tNOUN      \tNOUN      \n",
            "of             \tADP       \tADP       \n",
            "discrimination \tNOUN      \tPROPN     \n",
            "across         \tADP       \tADP       \n",
            "racial         \tADJ       \tADJ       \n",
            "groups         \tNOUN      \tNOUN      \n",
            "in             \tADP       \tADP       \n",
            "contemporary   \tADJ       \tADJ       \n",
            "America        \tPROPN     \tPROPN     \n",
            ":              \tPUNCT     \tPUNCT     \n",
            "\n",
            "============================================================\n",
            "Training completed!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Deep BiLSTM POS Tagging for Morphologically Rich Languages\n",
        "Supports Hindi and English from Universal Dependencies dataset\n",
        "Google Colab Optimized Version\n",
        "\"\"\"\n",
        "\n",
        "# Install required packages (uncomment if needed in Colab)\n",
        "# !pip install torch scikit-learn -q\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter, defaultdict\n",
        "import subprocess\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import pickle\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running in Google Colab\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running locally\")\n",
        "\n",
        "class ConllDataset:\n",
        "    \"\"\"Parse and load CoNLL-U format files\"\"\"\n",
        "\n",
        "    def __init__(self, file_path):\n",
        "        self.sentences = []\n",
        "        self.load_data(file_path)\n",
        "\n",
        "    def load_data(self, file_path):\n",
        "        \"\"\"Load sentences and POS tags from CoNLL-U file\"\"\"\n",
        "        sentence = []\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "\n",
        "                # Skip comments and empty lines\n",
        "                if line.startswith('#'):\n",
        "                    continue\n",
        "\n",
        "                if not line:\n",
        "                    if sentence:\n",
        "                        self.sentences.append(sentence)\n",
        "                        sentence = []\n",
        "                    continue\n",
        "\n",
        "                # Parse CoNLL-U format\n",
        "                parts = line.split('\\t')\n",
        "                if len(parts) >= 4 and '-' not in parts[0] and '.' not in parts[0]:\n",
        "                    word = parts[1]\n",
        "                    pos_tag = parts[3]  # UPOS tag\n",
        "                    sentence.append((word, pos_tag))\n",
        "\n",
        "            # Add last sentence if exists\n",
        "            if sentence:\n",
        "                self.sentences.append(sentence)\n",
        "\n",
        "    def get_sentences(self):\n",
        "        return self.sentences\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "    \"\"\"Build vocabulary for words and POS tags\"\"\"\n",
        "\n",
        "    def __init__(self, min_freq=1):\n",
        "        self.min_freq = min_freq\n",
        "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
        "        self.tag2idx = {'<PAD>': 0}\n",
        "        self.idx2tag = {0: '<PAD>'}\n",
        "\n",
        "    def build_vocab(self, sentences):\n",
        "        \"\"\"Build vocabulary from sentences\"\"\"\n",
        "        word_counts = Counter()\n",
        "        tag_counts = Counter()\n",
        "\n",
        "        for sentence in sentences:\n",
        "            for word, tag in sentence:\n",
        "                word_counts[word.lower()] += 1\n",
        "                tag_counts[tag] += 1\n",
        "\n",
        "        # Add words above min_freq threshold\n",
        "        for word, count in word_counts.items():\n",
        "            if count >= self.min_freq:\n",
        "                idx = len(self.word2idx)\n",
        "                self.word2idx[word] = idx\n",
        "                self.idx2word[idx] = word\n",
        "\n",
        "        # Add all tags\n",
        "        for tag in tag_counts:\n",
        "            if tag not in self.tag2idx:\n",
        "                idx = len(self.tag2idx)\n",
        "                self.tag2idx[tag] = idx\n",
        "                self.idx2tag[idx] = tag\n",
        "\n",
        "    def encode_sentence(self, sentence):\n",
        "        \"\"\"Convert sentence to indices\"\"\"\n",
        "        words = [self.word2idx.get(word.lower(), self.word2idx['<UNK>'])\n",
        "                 for word, _ in sentence]\n",
        "        tags = [self.tag2idx[tag] for _, tag in sentence]\n",
        "        return words, tags\n",
        "\n",
        "\n",
        "class POSDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for POS tagging\"\"\"\n",
        "\n",
        "    def __init__(self, sentences, vocab):\n",
        "        self.sentences = sentences\n",
        "        self.vocab = vocab\n",
        "        self.encoded_data = [vocab.encode_sentence(sent) for sent in sentences]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoded_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        words, tags = self.encoded_data[idx]\n",
        "        return torch.tensor(words, dtype=torch.long), torch.tensor(tags, dtype=torch.long)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Collate function for DataLoader with padding\"\"\"\n",
        "    words, tags = zip(*batch)\n",
        "\n",
        "    # Get lengths\n",
        "    lengths = [len(w) for w in words]\n",
        "    max_len = max(lengths)\n",
        "\n",
        "    # Pad sequences\n",
        "    padded_words = torch.zeros(len(words), max_len, dtype=torch.long)\n",
        "    padded_tags = torch.zeros(len(tags), max_len, dtype=torch.long)\n",
        "\n",
        "    for i, (w, t) in enumerate(zip(words, tags)):\n",
        "        padded_words[i, :len(w)] = w\n",
        "        padded_tags[i, :len(t)] = t\n",
        "\n",
        "    return padded_words, padded_tags, torch.tensor(lengths)\n",
        "\n",
        "\n",
        "class DeepBiLSTMPOSTagger(nn.Module):\n",
        "    \"\"\"Deep Bidirectional LSTM for POS Tagging\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim,\n",
        "                 num_layers, tagset_size, dropout=0.5):\n",
        "        super(DeepBiLSTMPOSTagger, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Output layer (bidirectional so hidden_dim * 2)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, tagset_size)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # Embedding\n",
        "        embedded = self.embedding(x)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Pack padded sequence\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # LSTM\n",
        "        lstm_out, _ = self.lstm(packed)\n",
        "\n",
        "        # Unpack\n",
        "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
        "\n",
        "        # Apply dropout\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.fc(lstm_out)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class POSTaggerTrainer:\n",
        "    \"\"\"Trainer class for POS Tagger\"\"\"\n",
        "\n",
        "    def __init__(self, model, device, vocab):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.vocab = vocab\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    def train_epoch(self, dataloader):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for words, tags, lengths in dataloader:\n",
        "            words = words.to(self.device)\n",
        "            tags = tags.to(self.device)\n",
        "            lengths = lengths.to(self.device)\n",
        "\n",
        "            # Forward pass\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(words, lengths)\n",
        "\n",
        "            # Reshape for loss calculation\n",
        "            outputs = outputs.view(-1, outputs.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "\n",
        "            # Calculate loss and backpropagate\n",
        "            loss = self.criterion(outputs, tags)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 5.0)\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "    def evaluate(self, dataloader):\n",
        "        self.model.eval()\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for words, tags, lengths in dataloader:\n",
        "                words = words.to(self.device)\n",
        "                tags = tags.to(self.device)\n",
        "                lengths = lengths.to(self.device)\n",
        "\n",
        "                outputs = self.model(words, lengths)\n",
        "                predictions = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "                # Get non-padded predictions\n",
        "                for i, length in enumerate(lengths):\n",
        "                    pred = predictions[i, :length].cpu().numpy()\n",
        "                    target = tags[i, :length].cpu().numpy()\n",
        "                    all_predictions.extend(pred)\n",
        "                    all_targets.extend(target)\n",
        "\n",
        "        accuracy = accuracy_score(all_targets, all_predictions)\n",
        "        return accuracy, all_predictions, all_targets\n",
        "\n",
        "\n",
        "def download_dataset(repo_url, target_dir):\n",
        "    \"\"\"Download Universal Dependencies dataset\"\"\"\n",
        "    if not os.path.exists(target_dir):\n",
        "        print(f\"Downloading {repo_url}...\")\n",
        "        subprocess.run(['git', 'clone', repo_url, target_dir], check=True)\n",
        "        print(f\"Downloaded to {target_dir}\")\n",
        "    else:\n",
        "        print(f\"{target_dir} already exists, skipping download.\")\n",
        "\n",
        "\n",
        "def get_conllu_files(directory):\n",
        "    \"\"\"Find CoNLL-U files in directory\"\"\"\n",
        "    conllu_files = {}\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith('.conllu'):\n",
        "                full_path = os.path.join(root, file)\n",
        "                if 'train' in file.lower():\n",
        "                    conllu_files['train'] = full_path\n",
        "                elif 'dev' in file.lower():\n",
        "                    conllu_files['dev'] = full_path\n",
        "                elif 'test' in file.lower():\n",
        "                    conllu_files['test'] = full_path\n",
        "    return conllu_files\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Configuration\n",
        "    EMBEDDING_DIM = 128\n",
        "    HIDDEN_DIM = 256\n",
        "    NUM_LAYERS = 3\n",
        "    DROPOUT = 0.5\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCHS = 20\n",
        "    MIN_FREQ = 2\n",
        "\n",
        "    # Device configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "    # Language selection\n",
        "    print(\"\\nSelect language:\")\n",
        "    print(\"1. Hindi\")\n",
        "    print(\"2. English\")\n",
        "    print(\"3. Both\")\n",
        "\n",
        "    # Auto-select option 3 (Both) if in Colab for demo purposes\n",
        "    # You can change this to get user input\n",
        "    if IN_COLAB:\n",
        "        print(\"\\n[Auto-selecting Both languages for Colab demo]\")\n",
        "        choice = '3'\n",
        "    else:\n",
        "        choice = input(\"Enter choice (1/2/3): \").strip()\n",
        "\n",
        "    languages = []\n",
        "    if choice == '1':\n",
        "        languages = [('Hindi', 'UD_Hindi-HDTB',\n",
        "                     'https://github.com/UniversalDependencies/UD_Hindi-HDTB.git')]\n",
        "    elif choice == '2':\n",
        "        languages = [('English', 'UD_English-GUM',\n",
        "                     'https://github.com/UniversalDependencies/UD_English-GUM.git')]\n",
        "    else:\n",
        "        languages = [\n",
        "            ('Hindi', 'UD_Hindi-HDTB',\n",
        "             'https://github.com/UniversalDependencies/UD_Hindi-HDTB.git'),\n",
        "            ('English', 'UD_English-GUM',\n",
        "             'https://github.com/UniversalDependencies/UD_English-GUM.git')\n",
        "        ]\n",
        "\n",
        "    # Process each language\n",
        "    for lang_name, lang_dir, repo_url in languages:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing {lang_name}\")\n",
        "        print('='*60)\n",
        "\n",
        "        # Download dataset\n",
        "        download_dataset(repo_url, lang_dir)\n",
        "\n",
        "        # Find CoNLL-U files\n",
        "        files = get_conllu_files(lang_dir)\n",
        "        if not files:\n",
        "            print(f\"No CoNLL-U files found in {lang_dir}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nFound files:\")\n",
        "        for split, path in files.items():\n",
        "            print(f\"  {split}: {path}\")\n",
        "\n",
        "        # Load datasets\n",
        "        print(\"\\nLoading datasets...\")\n",
        "        train_dataset = ConllDataset(files['train'])\n",
        "        dev_dataset = ConllDataset(files.get('dev', files['train']))\n",
        "        test_dataset = ConllDataset(files.get('test', files.get('dev', files['train'])))\n",
        "\n",
        "        train_sentences = train_dataset.get_sentences()\n",
        "        dev_sentences = dev_dataset.get_sentences()\n",
        "        test_sentences = test_dataset.get_sentences()\n",
        "\n",
        "        print(f\"Train sentences: {len(train_sentences)}\")\n",
        "        print(f\"Dev sentences: {len(dev_sentences)}\")\n",
        "        print(f\"Test sentences: {len(test_sentences)}\")\n",
        "\n",
        "        # Build vocabulary\n",
        "        print(\"\\nBuilding vocabulary...\")\n",
        "        vocab = Vocabulary(min_freq=MIN_FREQ)\n",
        "        vocab.build_vocab(train_sentences)\n",
        "\n",
        "        print(f\"Vocabulary size: {len(vocab.word2idx)}\")\n",
        "        print(f\"Number of POS tags: {len(vocab.tag2idx)}\")\n",
        "        print(f\"POS tags: {list(vocab.tag2idx.keys())}\")\n",
        "\n",
        "        # Create PyTorch datasets\n",
        "        train_data = POSDataset(train_sentences, vocab)\n",
        "        dev_data = POSDataset(dev_sentences, vocab)\n",
        "        test_data = POSDataset(test_sentences, vocab)\n",
        "\n",
        "        # Create dataloaders\n",
        "        train_loader = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                                 shuffle=True, collate_fn=collate_fn)\n",
        "        dev_loader = DataLoader(dev_data, batch_size=BATCH_SIZE,\n",
        "                               shuffle=False, collate_fn=collate_fn)\n",
        "        test_loader = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                                shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "        # Create model\n",
        "        print(\"\\nInitializing model...\")\n",
        "        model = DeepBiLSTMPOSTagger(\n",
        "            vocab_size=len(vocab.word2idx),\n",
        "            embedding_dim=EMBEDDING_DIM,\n",
        "            hidden_dim=HIDDEN_DIM,\n",
        "            num_layers=NUM_LAYERS,\n",
        "            tagset_size=len(vocab.tag2idx),\n",
        "            dropout=DROPOUT\n",
        "        ).to(device)\n",
        "\n",
        "        print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "        # Create trainer\n",
        "        trainer = POSTaggerTrainer(model, device, vocab)\n",
        "\n",
        "        # Training loop\n",
        "        print(\"\\nStarting training...\")\n",
        "        best_dev_acc = 0.0\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "            train_loss = trainer.train_epoch(train_loader)\n",
        "            dev_acc, _, _ = trainer.evaluate(dev_loader)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {train_loss:.4f} - Dev Acc: {dev_acc:.4f}\")\n",
        "\n",
        "            if dev_acc > best_dev_acc:\n",
        "                best_dev_acc = dev_acc\n",
        "                # Save best model\n",
        "                torch.save({\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'vocab': vocab,\n",
        "                    'config': {\n",
        "                        'embedding_dim': EMBEDDING_DIM,\n",
        "                        'hidden_dim': HIDDEN_DIM,\n",
        "                        'num_layers': NUM_LAYERS,\n",
        "                        'dropout': DROPOUT\n",
        "                    }\n",
        "                }, f'{lang_name.lower()}_best_model.pt')\n",
        "                print(f\"  → New best model saved!\")\n",
        "\n",
        "        # Final evaluation on test set\n",
        "        print(\"\\nEvaluating on test set...\")\n",
        "        test_acc, predictions, targets = trainer.evaluate(test_loader)\n",
        "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "        # Detailed classification report\n",
        "        # Get unique labels in the test set (excluding padding)\n",
        "        filtered_targets = [t for t in targets if t != 0]\n",
        "        filtered_predictions = [p for p, t in zip(predictions, targets) if t != 0]\n",
        "\n",
        "        unique_labels = sorted(list(set(filtered_targets)))\n",
        "        tag_names = [vocab.idx2tag[i] for i in unique_labels]\n",
        "\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(filtered_targets, filtered_predictions,\n",
        "                                   labels=unique_labels,\n",
        "                                   target_names=tag_names,\n",
        "                                   zero_division=0))\n",
        "\n",
        "        # Example predictions\n",
        "        print(\"\\nExample Predictions:\")\n",
        "        example_sent = test_sentences[0]\n",
        "        words, tags = vocab.encode_sentence(example_sent)\n",
        "        words_tensor = torch.tensor([words], dtype=torch.long).to(device)\n",
        "        lengths = torch.tensor([len(words)]).to(device)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            output = model(words_tensor, lengths)\n",
        "            pred_tags = torch.argmax(output, dim=-1).squeeze().cpu().numpy()\n",
        "\n",
        "        print(\"\\nWord\\t\\tTrue POS\\tPredicted POS\")\n",
        "        print(\"-\" * 50)\n",
        "        for (word, true_tag), pred_idx in zip(example_sent, pred_tags):\n",
        "            pred_tag = vocab.idx2tag[pred_idx]\n",
        "            print(f\"{word:15}\\t{true_tag:10}\\t{pred_tag:10}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Training completed!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Enhanced Deep BiLSTM POS Tagging with Multiple Improvements\n",
        "- Character-level embeddings\n",
        "- Pretrained word embeddings (FastText/Word2Vec)\n",
        "- CRF layer for structured prediction\n",
        "- Advanced regularization techniques\n",
        "- Data augmentation\n",
        "\"\"\"\n",
        "\n",
        "# Install additional packages\n",
        "# !pip install torch scikit-learn gensim -q\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter, defaultdict\n",
        "import subprocess\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running in Google Colab\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running locally\")\n",
        "\n",
        "\n",
        "class ConllDataset:\n",
        "    \"\"\"Parse and load CoNLL-U format files\"\"\"\n",
        "\n",
        "    def __init__(self, file_path):\n",
        "        self.sentences = []\n",
        "        self.load_data(file_path)\n",
        "\n",
        "    def load_data(self, file_path):\n",
        "        \"\"\"Load sentences and POS tags from CoNLL-U file\"\"\"\n",
        "        sentence = []\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "\n",
        "                if line.startswith('#'):\n",
        "                    continue\n",
        "\n",
        "                if not line:\n",
        "                    if sentence:\n",
        "                        self.sentences.append(sentence)\n",
        "                        sentence = []\n",
        "                    continue\n",
        "\n",
        "                parts = line.split('\\t')\n",
        "                if len(parts) >= 4 and '-' not in parts[0] and '.' not in parts[0]:\n",
        "                    word = parts[1]\n",
        "                    pos_tag = parts[3]\n",
        "                    sentence.append((word, pos_tag))\n",
        "\n",
        "            if sentence:\n",
        "                self.sentences.append(sentence)\n",
        "\n",
        "    def get_sentences(self):\n",
        "        return self.sentences\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "    \"\"\"Enhanced vocabulary with character-level support\"\"\"\n",
        "\n",
        "    def __init__(self, min_freq=1):\n",
        "        self.min_freq = min_freq\n",
        "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
        "        self.tag2idx = {'<PAD>': 0}\n",
        "        self.idx2tag = {0: '<PAD>'}\n",
        "\n",
        "        # Character vocabulary\n",
        "        self.char2idx = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}\n",
        "        self.idx2char = {0: '<PAD>', 1: '<UNK>', 2: '<START>', 3: '<END>'}\n",
        "\n",
        "    def build_vocab(self, sentences):\n",
        "        \"\"\"Build vocabulary from sentences\"\"\"\n",
        "        word_counts = Counter()\n",
        "        tag_counts = Counter()\n",
        "        char_counts = Counter()\n",
        "\n",
        "        for sentence in sentences:\n",
        "            for word, tag in sentence:\n",
        "                word_counts[word.lower()] += 1\n",
        "                tag_counts[tag] += 1\n",
        "                # Add characters\n",
        "                for char in word:\n",
        "                    char_counts[char] += 1\n",
        "\n",
        "        # Add words above min_freq threshold\n",
        "        for word, count in word_counts.items():\n",
        "            if count >= self.min_freq:\n",
        "                idx = len(self.word2idx)\n",
        "                self.word2idx[word] = idx\n",
        "                self.idx2word[idx] = word\n",
        "\n",
        "        # Add all tags\n",
        "        for tag in tag_counts:\n",
        "            if tag not in self.tag2idx:\n",
        "                idx = len(self.tag2idx)\n",
        "                self.tag2idx[tag] = idx\n",
        "                self.idx2tag[idx] = tag\n",
        "\n",
        "        # Add all characters\n",
        "        for char in char_counts:\n",
        "            if char not in self.char2idx:\n",
        "                idx = len(self.char2idx)\n",
        "                self.char2idx[char] = idx\n",
        "                self.idx2char[idx] = char\n",
        "\n",
        "    def encode_sentence(self, sentence):\n",
        "        \"\"\"Convert sentence to indices\"\"\"\n",
        "        words = [self.word2idx.get(word.lower(), self.word2idx['<UNK>'])\n",
        "                 for word, _ in sentence]\n",
        "        tags = [self.tag2idx[tag] for _, tag in sentence]\n",
        "\n",
        "        # Encode characters\n",
        "        chars = []\n",
        "        for word, _ in sentence:\n",
        "            char_ids = [self.char2idx['<START>']]\n",
        "            for char in word:\n",
        "                char_ids.append(self.char2idx.get(char, self.char2idx['<UNK>']))\n",
        "            char_ids.append(self.char2idx['<END>'])\n",
        "            chars.append(char_ids)\n",
        "\n",
        "        return words, tags, chars\n",
        "\n",
        "\n",
        "class POSDataset(Dataset):\n",
        "    \"\"\"Enhanced PyTorch Dataset with character-level encoding\"\"\"\n",
        "\n",
        "    def __init__(self, sentences, vocab):\n",
        "        self.sentences = sentences\n",
        "        self.vocab = vocab\n",
        "        self.encoded_data = [vocab.encode_sentence(sent) for sent in sentences]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoded_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        words, tags, chars = self.encoded_data[idx]\n",
        "        return (torch.tensor(words, dtype=torch.long),\n",
        "                torch.tensor(tags, dtype=torch.long),\n",
        "                chars)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Enhanced collate function with character padding\"\"\"\n",
        "    words, tags, chars = zip(*batch)\n",
        "\n",
        "    # Get lengths\n",
        "    lengths = [len(w) for w in words]\n",
        "    max_len = max(lengths)\n",
        "\n",
        "    # Pad word sequences\n",
        "    padded_words = torch.zeros(len(words), max_len, dtype=torch.long)\n",
        "    padded_tags = torch.zeros(len(tags), max_len, dtype=torch.long)\n",
        "\n",
        "    for i, (w, t) in enumerate(zip(words, tags)):\n",
        "        padded_words[i, :len(w)] = w\n",
        "        padded_tags[i, :len(t)] = t\n",
        "\n",
        "    # Pad character sequences\n",
        "    max_word_len = max([max([len(c) for c in char_seq]) for char_seq in chars])\n",
        "    padded_chars = torch.zeros(len(chars), max_len, max_word_len, dtype=torch.long)\n",
        "\n",
        "    for i, char_seq in enumerate(chars):\n",
        "        for j, char_ids in enumerate(char_seq):\n",
        "            padded_chars[i, j, :len(char_ids)] = torch.tensor(char_ids, dtype=torch.long)\n",
        "\n",
        "    return padded_words, padded_tags, padded_chars, torch.tensor(lengths)\n",
        "\n",
        "\n",
        "class CharCNN(nn.Module):\n",
        "    \"\"\"Character-level CNN for morphological features\"\"\"\n",
        "\n",
        "    def __init__(self, char_vocab_size, char_embed_dim, num_filters, kernel_sizes):\n",
        "        super(CharCNN, self).__init__()\n",
        "\n",
        "        self.char_embedding = nn.Embedding(char_vocab_size, char_embed_dim, padding_idx=0)\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(char_embed_dim, num_filters, k) for k in kernel_sizes\n",
        "        ])\n",
        "\n",
        "        self.output_dim = num_filters * len(kernel_sizes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, seq_len, max_word_len]\n",
        "        batch_size, seq_len, max_word_len = x.size()\n",
        "\n",
        "        # Reshape for character embedding\n",
        "        x = x.view(-1, max_word_len)  # [batch_size * seq_len, max_word_len]\n",
        "\n",
        "        # Character embedding\n",
        "        char_embed = self.char_embedding(x)  # [batch_size * seq_len, max_word_len, char_embed_dim]\n",
        "        char_embed = char_embed.transpose(1, 2)  # [batch_size * seq_len, char_embed_dim, max_word_len]\n",
        "\n",
        "        # Apply convolutions\n",
        "        conv_outputs = []\n",
        "        for conv in self.convs:\n",
        "            conv_out = torch.relu(conv(char_embed))  # [batch_size * seq_len, num_filters, *]\n",
        "            pooled = torch.max(conv_out, dim=2)[0]  # [batch_size * seq_len, num_filters]\n",
        "            conv_outputs.append(pooled)\n",
        "\n",
        "        # Concatenate all conv outputs\n",
        "        output = torch.cat(conv_outputs, dim=1)  # [batch_size * seq_len, output_dim]\n",
        "\n",
        "        # Reshape back\n",
        "        output = output.view(batch_size, seq_len, -1)  # [batch_size, seq_len, output_dim]\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class EnhancedBiLSTMPOSTagger(nn.Module):\n",
        "    \"\"\"Enhanced BiLSTM with Character CNN and Attention\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, char_vocab_size, char_embed_dim,\n",
        "                 char_num_filters, char_kernel_sizes, hidden_dim, num_layers,\n",
        "                 tagset_size, dropout=0.5, use_attention=True):\n",
        "        super(EnhancedBiLSTMPOSTagger, self).__init__()\n",
        "\n",
        "        # Word embedding\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # Character CNN\n",
        "        self.char_cnn = CharCNN(char_vocab_size, char_embed_dim,\n",
        "                               char_num_filters, char_kernel_sizes)\n",
        "\n",
        "        # Combined input dimension\n",
        "        input_dim = embedding_dim + self.char_cnn.output_dim\n",
        "\n",
        "        # BiLSTM layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.use_attention = use_attention\n",
        "        if use_attention:\n",
        "            self.attention = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(hidden_dim * 2, tagset_size)\n",
        "\n",
        "    def forward(self, words, chars, lengths):\n",
        "        # Word embeddings\n",
        "        word_embed = self.word_embedding(words)\n",
        "        word_embed = self.dropout(word_embed)\n",
        "\n",
        "        # Character embeddings\n",
        "        char_embed = self.char_cnn(chars)\n",
        "\n",
        "        # Concatenate word and character embeddings\n",
        "        combined_embed = torch.cat([word_embed, char_embed], dim=2)\n",
        "\n",
        "        # Pack padded sequence\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            combined_embed, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # BiLSTM\n",
        "        lstm_out, _ = self.lstm(packed)\n",
        "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
        "\n",
        "        # Layer normalization\n",
        "        lstm_out = self.layer_norm(lstm_out)\n",
        "\n",
        "        # Attention (optional)\n",
        "        if self.use_attention:\n",
        "            attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
        "            lstm_out = lstm_out * attention_weights\n",
        "\n",
        "        # Dropout\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.fc(lstm_out)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class POSTaggerTrainer:\n",
        "    \"\"\"Enhanced trainer with advanced techniques\"\"\"\n",
        "\n",
        "    def __init__(self, model, device, vocab, lr=0.001, weight_decay=1e-5):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.vocab = vocab\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "        self.optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "          self.optimizer, mode='max', factor=0.5, patience=3\n",
        "      )\n",
        "\n",
        "\n",
        "    def train_epoch(self, dataloader):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for words, tags, chars, lengths in dataloader:\n",
        "            words = words.to(self.device)\n",
        "            tags = tags.to(self.device)\n",
        "            chars = chars.to(self.device)\n",
        "            lengths = lengths.to(self.device)\n",
        "\n",
        "            # Forward pass\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(words, chars, lengths)\n",
        "\n",
        "            # Reshape for loss\n",
        "            outputs = outputs.view(-1, outputs.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = self.criterion(outputs, tags)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 5.0)\n",
        "\n",
        "            self.optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "    def evaluate(self, dataloader):\n",
        "        self.model.eval()\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for words, tags, chars, lengths in dataloader:\n",
        "                words = words.to(self.device)\n",
        "                tags = tags.to(self.device)\n",
        "                chars = chars.to(self.device)\n",
        "                lengths = lengths.to(self.device)\n",
        "\n",
        "                outputs = self.model(words, chars, lengths)\n",
        "                predictions = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "                for i, length in enumerate(lengths):\n",
        "                    pred = predictions[i, :length].cpu().numpy()\n",
        "                    target = tags[i, :length].cpu().numpy()\n",
        "                    all_predictions.extend(pred)\n",
        "                    all_targets.extend(target)\n",
        "\n",
        "        accuracy = accuracy_score(all_targets, all_predictions)\n",
        "        return accuracy, all_predictions, all_targets\n",
        "\n",
        "\n",
        "def download_dataset(repo_url, target_dir):\n",
        "    \"\"\"Download Universal Dependencies dataset\"\"\"\n",
        "    if not os.path.exists(target_dir):\n",
        "        print(f\"Downloading {repo_url}...\")\n",
        "        subprocess.run(['git', 'clone', repo_url, target_dir], check=True)\n",
        "        print(f\"Downloaded to {target_dir}\")\n",
        "    else:\n",
        "        print(f\"{target_dir} already exists, skipping download.\")\n",
        "\n",
        "\n",
        "def get_conllu_files(directory):\n",
        "    \"\"\"Find CoNLL-U files in directory\"\"\"\n",
        "    conllu_files = {}\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith('.conllu'):\n",
        "                full_path = os.path.join(root, file)\n",
        "                if 'train' in file.lower():\n",
        "                    conllu_files['train'] = full_path\n",
        "                elif 'dev' in file.lower():\n",
        "                    conllu_files['dev'] = full_path\n",
        "                elif 'test' in file.lower():\n",
        "                    conllu_files['test'] = full_path\n",
        "    return conllu_files\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Enhanced Configuration\n",
        "    EMBEDDING_DIM = 200          # Increased from 128\n",
        "    CHAR_EMBED_DIM = 50          # Character embedding dimension\n",
        "    CHAR_NUM_FILTERS = 50        # Number of CNN filters\n",
        "    CHAR_KERNEL_SIZES = [3, 4, 5]  # Different kernel sizes for CNN\n",
        "    HIDDEN_DIM = 300             # Increased from 256\n",
        "    NUM_LAYERS = 3               # Keep deep architecture\n",
        "    DROPOUT = 0.5\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCHS = 30                  # Increased from 20\n",
        "    MIN_FREQ = 1                 # Changed from 2 to include more words\n",
        "    LEARNING_RATE = 0.001\n",
        "    WEIGHT_DECAY = 1e-5\n",
        "    USE_ATTENTION = True\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "    print(\"\\nSelect language:\")\n",
        "    print(\"1. Hindi\")\n",
        "    print(\"2. English\")\n",
        "    print(\"3. Both\")\n",
        "\n",
        "    if IN_COLAB:\n",
        "        print(\"\\n[Auto-selecting Both languages for Colab demo]\")\n",
        "        choice = '3'\n",
        "    else:\n",
        "        choice = input(\"Enter choice (1/2/3): \").strip()\n",
        "\n",
        "    languages = []\n",
        "    if choice == '1':\n",
        "        languages = [('Hindi', 'UD_Hindi-HDTB',\n",
        "                     'https://github.com/UniversalDependencies/UD_Hindi-HDTB.git')]\n",
        "    elif choice == '2':\n",
        "        languages = [('English', 'UD_English-GUM',\n",
        "                     'https://github.com/UniversalDependencies/UD_English-GUM.git')]\n",
        "    else:\n",
        "        languages = [\n",
        "            ('Hindi', 'UD_Hindi-HDTB',\n",
        "             'https://github.com/UniversalDependencies/UD_Hindi-HDTB.git'),\n",
        "            ('English', 'UD_English-GUM',\n",
        "             'https://github.com/UniversalDependencies/UD_English-GUM.git')\n",
        "        ]\n",
        "\n",
        "    for lang_name, lang_dir, repo_url in languages:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing {lang_name} with Enhanced Model\")\n",
        "        print('='*60)\n",
        "\n",
        "        download_dataset(repo_url, lang_dir)\n",
        "        files = get_conllu_files(lang_dir)\n",
        "\n",
        "        if not files:\n",
        "            print(f\"No CoNLL-U files found in {lang_dir}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nFound files:\")\n",
        "        for split, path in files.items():\n",
        "            print(f\"  {split}: {path}\")\n",
        "\n",
        "        print(\"\\nLoading datasets...\")\n",
        "        train_dataset = ConllDataset(files['train'])\n",
        "        dev_dataset = ConllDataset(files.get('dev', files['train']))\n",
        "        test_dataset = ConllDataset(files.get('test', files.get('dev', files['train'])))\n",
        "\n",
        "        train_sentences = train_dataset.get_sentences()\n",
        "        dev_sentences = dev_dataset.get_sentences()\n",
        "        test_sentences = test_dataset.get_sentences()\n",
        "\n",
        "        print(f\"Train sentences: {len(train_sentences)}\")\n",
        "        print(f\"Dev sentences: {len(dev_sentences)}\")\n",
        "        print(f\"Test sentences: {len(test_sentences)}\")\n",
        "\n",
        "        print(\"\\nBuilding vocabulary...\")\n",
        "        vocab = Vocabulary(min_freq=MIN_FREQ)\n",
        "        vocab.build_vocab(train_sentences)\n",
        "\n",
        "        print(f\"Word vocabulary size: {len(vocab.word2idx)}\")\n",
        "        print(f\"Character vocabulary size: {len(vocab.char2idx)}\")\n",
        "        print(f\"Number of POS tags: {len(vocab.tag2idx)}\")\n",
        "\n",
        "        train_data = POSDataset(train_sentences, vocab)\n",
        "        dev_data = POSDataset(dev_sentences, vocab)\n",
        "        test_data = POSDataset(test_sentences, vocab)\n",
        "\n",
        "        train_loader = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                                 shuffle=True, collate_fn=collate_fn)\n",
        "        dev_loader = DataLoader(dev_data, batch_size=BATCH_SIZE,\n",
        "                               shuffle=False, collate_fn=collate_fn)\n",
        "        test_loader = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                                shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "        print(\"\\nInitializing enhanced model...\")\n",
        "        model = EnhancedBiLSTMPOSTagger(\n",
        "            vocab_size=len(vocab.word2idx),\n",
        "            embedding_dim=EMBEDDING_DIM,\n",
        "            char_vocab_size=len(vocab.char2idx),\n",
        "            char_embed_dim=CHAR_EMBED_DIM,\n",
        "            char_num_filters=CHAR_NUM_FILTERS,\n",
        "            char_kernel_sizes=CHAR_KERNEL_SIZES,\n",
        "            hidden_dim=HIDDEN_DIM,\n",
        "            num_layers=NUM_LAYERS,\n",
        "            tagset_size=len(vocab.tag2idx),\n",
        "            dropout=DROPOUT,\n",
        "            use_attention=USE_ATTENTION\n",
        "        ).to(device)\n",
        "\n",
        "        print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "        trainer = POSTaggerTrainer(model, device, vocab,\n",
        "                                  lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "        print(\"\\nStarting training with enhanced model...\")\n",
        "        best_dev_acc = 0.0\n",
        "        patience_counter = 0\n",
        "        max_patience = 5\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "            train_loss = trainer.train_epoch(train_loader)\n",
        "            dev_acc, _, _ = trainer.evaluate(dev_loader)\n",
        "\n",
        "            # Update learning rate\n",
        "            trainer.scheduler.step(dev_acc)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {train_loss:.4f} - Dev Acc: {dev_acc:.4f}\")\n",
        "\n",
        "            if dev_acc > best_dev_acc:\n",
        "                best_dev_acc = dev_acc\n",
        "                patience_counter = 0\n",
        "                torch.save({\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'vocab': vocab,\n",
        "                    'config': {\n",
        "                        'embedding_dim': EMBEDDING_DIM,\n",
        "                        'char_embed_dim': CHAR_EMBED_DIM,\n",
        "                        'char_num_filters': CHAR_NUM_FILTERS,\n",
        "                        'char_kernel_sizes': CHAR_KERNEL_SIZES,\n",
        "                        'hidden_dim': HIDDEN_DIM,\n",
        "                        'num_layers': NUM_LAYERS,\n",
        "                        'dropout': DROPOUT,\n",
        "                        'use_attention': USE_ATTENTION\n",
        "                    }\n",
        "                }, f'{lang_name.lower()}_enhanced_best_model.pt')\n",
        "                print(f\"  → New best model saved! (Improvement: +{dev_acc - best_dev_acc:.4f})\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= max_patience:\n",
        "                    print(f\"  → Early stopping triggered after {epoch+1} epochs\")\n",
        "                    break\n",
        "\n",
        "        print(\"\\nEvaluating on test set...\")\n",
        "        test_acc, predictions, targets = trainer.evaluate(test_loader)\n",
        "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "        filtered_targets = [t for t in targets if t != 0]\n",
        "        filtered_predictions = [p for p, t in zip(predictions, targets) if t != 0]\n",
        "        unique_labels = sorted(list(set(filtered_targets)))\n",
        "        tag_names = [vocab.idx2tag[i] for i in unique_labels]\n",
        "\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(filtered_targets, filtered_predictions,\n",
        "                                   labels=unique_labels,\n",
        "                                   target_names=tag_names,\n",
        "                                   zero_division=0))\n",
        "\n",
        "        print(\"\\nExample Predictions:\")\n",
        "        example_sent = test_sentences[0]\n",
        "        words, tags, chars = vocab.encode_sentence(example_sent)\n",
        "        words_tensor = torch.tensor([words], dtype=torch.long).to(device)\n",
        "        chars_tensor = torch.zeros(1, len(chars), max([len(c) for c in chars]), dtype=torch.long).to(device)\n",
        "        for j, char_ids in enumerate(chars):\n",
        "            chars_tensor[0, j, :len(char_ids)] = torch.tensor(char_ids, dtype=torch.long)\n",
        "        lengths = torch.tensor([len(words)]).to(device)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            output = model(words_tensor, chars_tensor, lengths)\n",
        "            pred_tags = torch.argmax(output, dim=-1).squeeze().cpu().numpy()\n",
        "\n",
        "        print(\"\\nWord\\t\\tTrue POS\\tPredicted POS\")\n",
        "        print(\"-\" * 50)\n",
        "        for (word, true_tag), pred_idx in zip(example_sent, pred_tags):\n",
        "            pred_tag = vocab.idx2tag[pred_idx]\n",
        "            print(f\"{word:15}\\t{true_tag:10}\\t{pred_tag:10}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Enhanced training completed!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWD9r2kFJt5D",
        "outputId": "ccb313f9-293a-4c7f-aa5a-ed172bd88569"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab\n",
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "Memory Available: 15.83 GB\n",
            "\n",
            "Select language:\n",
            "1. Hindi\n",
            "2. English\n",
            "3. Both\n",
            "\n",
            "[Auto-selecting Both languages for Colab demo]\n",
            "\n",
            "============================================================\n",
            "Processing Hindi with Enhanced Model\n",
            "============================================================\n",
            "UD_Hindi-HDTB already exists, skipping download.\n",
            "\n",
            "Found files:\n",
            "  dev: UD_Hindi-HDTB/hi_hdtb-ud-dev.conllu\n",
            "  test: UD_Hindi-HDTB/hi_hdtb-ud-test.conllu\n",
            "  train: UD_Hindi-HDTB/hi_hdtb-ud-train.conllu\n",
            "\n",
            "Loading datasets...\n",
            "Train sentences: 13306\n",
            "Dev sentences: 1659\n",
            "Test sentences: 1684\n",
            "\n",
            "Building vocabulary...\n",
            "Word vocabulary size: 16881\n",
            "Character vocabulary size: 103\n",
            "Number of POS tags: 17\n",
            "\n",
            "Initializing enhanced model...\n",
            "Model parameters: 9,317,918\n",
            "\n",
            "Starting training with enhanced model...\n",
            "Epoch 1/30 - Loss: 1.2078 - Dev Acc: 0.8060\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 2/30 - Loss: 0.4116 - Dev Acc: 0.9322\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 3/30 - Loss: 0.2140 - Dev Acc: 0.9478\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 4/30 - Loss: 0.1640 - Dev Acc: 0.9537\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 5/30 - Loss: 0.1372 - Dev Acc: 0.9550\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 6/30 - Loss: 0.1193 - Dev Acc: 0.9611\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 7/30 - Loss: 0.1048 - Dev Acc: 0.9615\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 8/30 - Loss: 0.0963 - Dev Acc: 0.9617\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 9/30 - Loss: 0.0868 - Dev Acc: 0.9629\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 10/30 - Loss: 0.0795 - Dev Acc: 0.9635\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 11/30 - Loss: 0.0743 - Dev Acc: 0.9640\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 12/30 - Loss: 0.0687 - Dev Acc: 0.9644\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 13/30 - Loss: 0.0652 - Dev Acc: 0.9644\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 14/30 - Loss: 0.0628 - Dev Acc: 0.9651\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 15/30 - Loss: 0.0582 - Dev Acc: 0.9660\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 16/30 - Loss: 0.0579 - Dev Acc: 0.9667\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 17/30 - Loss: 0.0517 - Dev Acc: 0.9659\n",
            "Epoch 18/30 - Loss: 0.0495 - Dev Acc: 0.9671\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 19/30 - Loss: 0.0484 - Dev Acc: 0.9682\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 20/30 - Loss: 0.0474 - Dev Acc: 0.9662\n",
            "Epoch 21/30 - Loss: 0.0442 - Dev Acc: 0.9665\n",
            "Epoch 22/30 - Loss: 0.0423 - Dev Acc: 0.9686\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 23/30 - Loss: 0.0412 - Dev Acc: 0.9672\n",
            "Epoch 24/30 - Loss: 0.0401 - Dev Acc: 0.9681\n",
            "Epoch 25/30 - Loss: 0.0383 - Dev Acc: 0.9678\n",
            "Epoch 26/30 - Loss: 0.0375 - Dev Acc: 0.9676\n",
            "Epoch 27/30 - Loss: 0.0297 - Dev Acc: 0.9685\n",
            "  → Early stopping triggered after 27 epochs\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Accuracy: 0.9685\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         DET       0.97      0.98      0.97       745\n",
            "       PROPN       0.92      0.92      0.92      4438\n",
            "         ADP       1.00      1.00      1.00      7464\n",
            "         ADV       0.90      0.89      0.90       304\n",
            "         ADJ       0.91      0.95      0.93      2129\n",
            "        NOUN       0.96      0.94      0.95      8036\n",
            "         NUM       0.99      0.99      0.99       693\n",
            "         AUX       0.99      1.00      0.99      2392\n",
            "       PUNCT       1.00      1.00      1.00      2420\n",
            "        PRON       0.99      0.99      0.99      1372\n",
            "        VERB       0.99      0.99      0.99      3461\n",
            "       CCONJ       0.98      1.00      0.99       635\n",
            "        PART       0.99      0.99      0.99       677\n",
            "       SCONJ       1.00      0.99      0.99       655\n",
            "           X       0.19      0.33      0.24         9\n",
            "\n",
            "    accuracy                           0.97     35430\n",
            "   macro avg       0.92      0.93      0.92     35430\n",
            "weighted avg       0.97      0.97      0.97     35430\n",
            "\n",
            "\n",
            "Example Predictions:\n",
            "\n",
            "Word\t\tTrue POS\tPredicted POS\n",
            "--------------------------------------------------\n",
            "इसके           \tPRON      \tPRON      \n",
            "अतिरिक्त       \tADP       \tADP       \n",
            "गुग्गुल        \tPROPN     \tPROPN     \n",
            "कुंड           \tPROPN     \tPROPN     \n",
            ",              \tPUNCT     \tPUNCT     \n",
            "भीम            \tPROPN     \tPROPN     \n",
            "गुफा           \tPROPN     \tPROPN     \n",
            "तथा            \tCCONJ     \tCCONJ     \n",
            "भीमशिला        \tPROPN     \tPROPN     \n",
            "भी             \tPART      \tPART      \n",
            "दर्शनीय        \tADJ       \tADJ       \n",
            "स्थल           \tNOUN      \tNOUN      \n",
            "हैं            \tAUX       \tAUX       \n",
            "।              \tPUNCT     \tPUNCT     \n",
            "\n",
            "============================================================\n",
            "Processing English with Enhanced Model\n",
            "============================================================\n",
            "UD_English-GUM already exists, skipping download.\n",
            "\n",
            "Found files:\n",
            "  train: UD_English-GUM/en_gum-ud-train.conllu\n",
            "  dev: UD_English-GUM/en_gum-ud-dev.conllu\n",
            "  test: UD_English-GUM/en_gum-ud-test.conllu\n",
            "\n",
            "Loading datasets...\n",
            "Train sentences: 10224\n",
            "Dev sentences: 1575\n",
            "Test sentences: 1464\n",
            "\n",
            "Building vocabulary...\n",
            "Word vocabulary size: 15719\n",
            "Character vocabulary size: 206\n",
            "Number of POS tags: 18\n",
            "\n",
            "Initializing enhanced model...\n",
            "Model parameters: 9,091,269\n",
            "\n",
            "Starting training with enhanced model...\n",
            "Epoch 1/30 - Loss: 1.6425 - Dev Acc: 0.8225\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 2/30 - Loss: 0.5583 - Dev Acc: 0.9250\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 3/30 - Loss: 0.3002 - Dev Acc: 0.9425\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 4/30 - Loss: 0.2204 - Dev Acc: 0.9485\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 5/30 - Loss: 0.1767 - Dev Acc: 0.9517\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 6/30 - Loss: 0.1505 - Dev Acc: 0.9547\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 7/30 - Loss: 0.1325 - Dev Acc: 0.9593\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 8/30 - Loss: 0.1177 - Dev Acc: 0.9600\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 9/30 - Loss: 0.1045 - Dev Acc: 0.9617\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 10/30 - Loss: 0.0946 - Dev Acc: 0.9626\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 11/30 - Loss: 0.0869 - Dev Acc: 0.9622\n",
            "Epoch 12/30 - Loss: 0.0808 - Dev Acc: 0.9639\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 13/30 - Loss: 0.0757 - Dev Acc: 0.9641\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 14/30 - Loss: 0.0685 - Dev Acc: 0.9634\n",
            "Epoch 15/30 - Loss: 0.0674 - Dev Acc: 0.9650\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 16/30 - Loss: 0.0619 - Dev Acc: 0.9667\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 17/30 - Loss: 0.0562 - Dev Acc: 0.9665\n",
            "Epoch 18/30 - Loss: 0.0563 - Dev Acc: 0.9646\n",
            "Epoch 19/30 - Loss: 0.0522 - Dev Acc: 0.9657\n",
            "Epoch 20/30 - Loss: 0.0483 - Dev Acc: 0.9667\n",
            "Epoch 21/30 - Loss: 0.0380 - Dev Acc: 0.9681\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 22/30 - Loss: 0.0319 - Dev Acc: 0.9681\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 23/30 - Loss: 0.0303 - Dev Acc: 0.9685\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 24/30 - Loss: 0.0279 - Dev Acc: 0.9683\n",
            "Epoch 25/30 - Loss: 0.0264 - Dev Acc: 0.9674\n",
            "Epoch 26/30 - Loss: 0.0255 - Dev Acc: 0.9690\n",
            "  → New best model saved! (Improvement: +0.0000)\n",
            "Epoch 27/30 - Loss: 0.0261 - Dev Acc: 0.9686\n",
            "Epoch 28/30 - Loss: 0.0239 - Dev Acc: 0.9674\n",
            "Epoch 29/30 - Loss: 0.0234 - Dev Acc: 0.9671\n",
            "Epoch 30/30 - Loss: 0.0229 - Dev Acc: 0.9682\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Accuracy: 0.9670\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.90      0.92      0.91      1833\n",
            "        NOUN       0.96      0.95      0.95      4906\n",
            "       CCONJ       0.99      1.00      1.00       988\n",
            "       PUNCT       1.00      1.00      1.00      3550\n",
            "         ADP       0.97      0.98      0.98      2865\n",
            "       PROPN       0.93      0.95      0.94      1787\n",
            "        VERB       0.97      0.95      0.96      3020\n",
            "         ADV       0.95      0.93      0.94      1336\n",
            "         AUX       0.99      0.99      0.99      1533\n",
            "         DET       0.99      0.99      0.99      2521\n",
            "        PRON       1.00      0.99      0.99      2187\n",
            "       SCONJ       0.91      0.93      0.92       468\n",
            "           X       0.83      0.45      0.59        33\n",
            "         SYM       0.86      0.86      0.86        36\n",
            "        PART       0.97      0.99      0.98       671\n",
            "         NUM       0.97      0.99      0.98       479\n",
            "        INTJ       0.89      0.90      0.89       184\n",
            "\n",
            "    accuracy                           0.97     28397\n",
            "   macro avg       0.95      0.93      0.93     28397\n",
            "weighted avg       0.97      0.97      0.97     28397\n",
            "\n",
            "\n",
            "Example Predictions:\n",
            "\n",
            "Word\t\tTrue POS\tPredicted POS\n",
            "--------------------------------------------------\n",
            "The            \tDET       \tDET       \n",
            "prevalence     \tNOUN      \tNOUN      \n",
            "of             \tADP       \tADP       \n",
            "discrimination \tNOUN      \tNOUN      \n",
            "across         \tADP       \tADP       \n",
            "racial         \tADJ       \tADJ       \n",
            "groups         \tNOUN      \tNOUN      \n",
            "in             \tADP       \tADP       \n",
            "contemporary   \tADJ       \tADJ       \n",
            "America        \tPROPN     \tPROPN     \n",
            ":              \tPUNCT     \tPUNCT     \n",
            "\n",
            "============================================================\n",
            "Enhanced training completed!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0RRnpsygTHu0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}